{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Numpy as py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "train = pd.read_csv('C:/Users/peter/Desktop/Home Default/application_train.csv')\n",
    "\n",
    "by_contract = train.groupby('NAME_CONTRACT_TYPE')\n",
    "by_contract_ = test.groupby('NAME_CONTRACT_TYPE')\n",
    "\n",
    "print(df.train['AMT_INCOME_TOTAL'].count())\n",
    "print(df.train['AMT_INCOME_TOTAL'].Mean())\n",
    "print(df.train['AMT_INCOME_TOTAL'].Median())\n",
    "print(df.train['AMT_INCOME_TOTAL'].Std())\n",
    "\n",
    "print(df.test['AMT_INCOME_TOTAL'].count())\n",
    "print(df.test['AMT_INCOME_TOTAL'].Mean())\n",
    "print(df.test['AMT_INCOME_TOTAL'].Median())\n",
    "print(df.test['AMT_INCOME_TOTAL'].Std())\n",
    "\n",
    "Income = df.train['NAME_CONTRACT_TYPE'].sort()\n",
    "Income_ = df.test['NAME_CONTRACT_TYPE'].sort()\n",
    "\n",
    "Income1 = df.train['AMT_INCOME_TOTAL'].sort()\n",
    "Income_1 = df.test['AMT_INCOME_TOTAL'].sort()\n",
    "\n",
    "print(Income.loc['NAME_CONTRACT_TYPE'])\n",
    "print(Income_ .loc['NAME_CONTRACT_TYPE'])\n",
    "\n",
    "\n",
    "#Train/Test Split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create feature and target arrays\n",
    "X = application_train['AMT_INCOME_TOTAL'].data\n",
    "y = application_train['AMT_CREDIT'].target\n",
    "\n",
    "#Correlation computation\n",
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(x, y)\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "print(df.train.describe())\n",
    "print(df.test.describe())\n",
    "\n",
    "#Create Dummy Variable\n",
    "df_car = pd.get_dummies(df.train['FLAG_OWN_CAR'])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import Imputer \n",
    "from sklearn.preprocessing import scale \n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "\n",
    "X = imp.transform(X) \n",
    "\n",
    "# Scaling in a pipeline\n",
    "steps = [('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())]\n",
    "\n",
    "pipeline = Pipeline(steps) \n",
    "knn_scaled = pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# CossValidation:\n",
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg, X, y, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys, c=labels,alpha=0.5)\n",
    "\n",
    "#Dimension Reduction\n",
    "\n",
    "import array \n",
    "arr= array.array(['samples',application_train.loc['AMT_INCOME_TOTAL','AMT_CREDIT']])\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "print(pca_features.shape)\n",
    "\n",
    "\n",
    "# Import scale\n",
    "from sklearn.preprocessing import scale\n",
    "# Scale the features: X_scaled\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Print the mean and standard deviation of the unscaled features\n",
    "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
    "\n",
    "# Print the mean and standard deviation of the scaled features\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
